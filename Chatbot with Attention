{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chatbot with Attention","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1BDOGuTqipLG1wTiLEQ6YqqzYn566DIHx","authorship_tag":"ABX9TyOBg4p4BohtFJoHjbITr2Pn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"oyEWED0KIqLV","colab_type":"text"},"source":["# **Load and process data**"]},{"cell_type":"code","metadata":{"id":"fRkV3_3mMNcX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1597545628168,"user_tz":240,"elapsed":2375,"user":{"displayName":"Hung Do","photoUrl":"","userId":"04577237231470330040"}},"outputId":"5bf39c85-fce1-431b-b837-4f707aca1da0"},"source":["import tensorflow as tf\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import os\n","import time\n","import json\n","\n","path_to_file = tf.keras.utils.get_file('movies.json', 'https://raw.githubusercontent.com/google-research-datasets/Taskmaster/master/TM-2-2020/data/movies.json')\n","\n","with open(path_to_file) as file:\n","  data = json.load(file)\n","\n","user = list()\n","assistant = list()\n","sentence = '<start>'\n","\n","for conversation in data:\n","  user_has_not_started = True\n","  # The conversation starts with the user speaks first\n","  user_is_talking = True\n","  assistant_is_talking = False\n","\n","  for utterance in conversation['utterances']:\n","    if utterance['speaker'] == 'ASSISTANT' and user_has_not_started:\n","      continue\n","    else:\n","      user_has_not_started = False\n","    \n","      # process the utterance  \n","      buffer = utterance['text']\n","      \n","      # These lines are for grouping special segments into one group. However,\n","      # that would require implementing another model for the machine to recognize\n","      # those model from the user. For the scope of this project, I'm going to pause here\n","\n","      if utterance.get('segments'):\n","        for segment in utterance.get('segments'):\n","          annotation = segment.get('annotations')[0]['name']\n","          annotation = '<' + annotation.replace('.','<>').replace('_','<>') + '>'\n","          buffer = buffer.replace(segment.get('text'), annotation)\n","      \n","      if utterance['speaker'] == 'USER':\n","        if assistant_is_talking:\n","          # finish assistant's sentence\n","          sentence = ' '.join(sentence.split(' ')[:-1]) + ' ' + '<end>'\n","          assistant.append(sentence)\n","          assistant_is_talking = False\n","\n","          # reset the sentence for user\n","          sentence = '<start>'\n","          user_is_talking = True\n","      \n","      if utterance['speaker'] == 'ASSISTANT':\n","        if user_is_talking:\n","          # finish user's sentence\n","          sentence = ' '.join(sentence.split(' ')[:-1]) + ' ' + '<end>'\n","          user.append(sentence)\n","          user_is_talking = False\n","\n","          # reset the sentence for assistant\n","          sentence = '<start>'\n","          assistant_is_talking = True\n","          \n","      # append to the sentence\n","      sentence = sentence + ' ' + buffer + ' ' + '<pause>'\n","  \n","  if assistant_is_talking:\n","    sentence = ' '.join(sentence.split(' ')[:-1]) + ' ' + '<end>'\n","    assistant.append(sentence)\n","  \n","print('Lenght User: {}'.format(len(user)))\n","print('Lenght Assistant: {}'.format(len(assistant)))"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Lenght User: 23961\n","Lenght Assistant: 23961\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lakv70fBUhKb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597545628173,"user_tz":240,"elapsed":2355,"user":{"displayName":"Hung Do","photoUrl":"","userId":"04577237231470330040"}},"outputId":"c903f303-589d-4536-ae7a-468b668b6030"},"source":["a = list()\n","for conversation in data:\n","  for utterance in conversation['utterances']:\n","    if utterance.get('segments'):\n","      for segment in utterance.get('segments'):\n","        a.extend(segment['annotations'])\n","\n","a = list(map(lambda x: x['name'].replace('.','<>').replace('_','<>'), a))#.split('.')[0] + '_' + x['name'].split('.')[1],a))\n","print(len(set(a)))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["26\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MYQvuw2JpDpO","colab_type":"text"},"source":["There are 26 different types of annotation in this dataset."]},{"cell_type":"code","metadata":{"id":"8z3f7r7EQjr_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":595},"executionInfo":{"status":"ok","timestamp":1597545630724,"user_tz":240,"elapsed":4891,"user":{"displayName":"Hung Do","photoUrl":"","userId":"04577237231470330040"}},"outputId":"b162899f-d567-4bb6-b5c7-274daa6616ae"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","tokenizer = Tokenizer(oov_token='<OOV>', filters='\\'!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n","tokenizer.fit_on_texts(assistant + user)\n","\n","word_index = tokenizer.word_index\n","\n","print('Vocabulary length: {}'.format(len(word_index)))\n","\n","encoder_input = tokenizer.texts_to_sequences(user)\n","decoder_input = tokenizer.texts_to_sequences(assistant)\n","target = list()\n","for inp in decoder_input:\n","  a = inp[1:]\n","  a.append(0)\n","  target.append(a)\n","\n","encoder_input = pad_sequences(encoder_input, padding='post')\n","decoder_input = pad_sequences(decoder_input, padding='post')\n","target = pad_sequences(target, padding='post')\n","\n","max_encoder_len = len(encoder_input[0])\n","max_decoder_len = len(decoder_input[0])\n","\n","print('Sample encoder:')\n","print(encoder_input[0])\n","print('Sample decoder:')\n","print(decoder_input[0])\n","print('Sample target:')\n","print(target[0])\n","print('Max length encoder: {}'.format(max_encoder_len))\n","print('Max length decoder: {}'.format(max_decoder_len))\n","print('Dataset size: {}'.format(len(encoder_input)))\n","\n","dataset = tf.data.Dataset.from_tensor_slices((encoder_input, decoder_input, target))\n","\n","# Batch size\n","BATCH_SIZE = 50\n","BUFFER_SIZE = 10000\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n","print(dataset.take(1))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Vocabulary length: 6580\n","Sample encoder:\n","[ 3 21 58 22 62  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n","Sample decoder:\n","[  3   5  26  30   4  45  37  10 236  11 148  22   4 231   2   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0]\n","Sample target:\n","[  5  26  30   4  45  37  10 236  11 148  22   4 231   2   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0]\n","Max length encoder: 140\n","Max length decoder: 171\n","Dataset size: 23961\n","<TakeDataset shapes: ((50, 140), (50, 171), (50, 171)), types: (tf.int32, tf.int32, tf.int32)>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5LUoGiXI8ha0"},"source":["# **LSTM with ATTENTION**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kSQLMC1Z8ha9","colab":{},"executionInfo":{"status":"ok","timestamp":1597545630725,"user_tz":240,"elapsed":4876,"user":{"displayName":"Hung Do","photoUrl":"","userId":"04577237231470330040"}}},"source":["EMBEDDING_DIM = 200\n","RNN_UNITS = 200\n","VOCAB_SIZE = len(word_index) + 1 # word_index count from 1, but keras layers count from 0"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"U7m-v4-AEQ9P","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597545630726,"user_tz":240,"elapsed":4865,"user":{"displayName":"Hung Do","photoUrl":"","userId":"04577237231470330040"}}},"source":["class Attention(tf.keras.layers.Layer):\n","  def __init__(self, attention_units):\n","    super().__init__()\n","    # Dense layer for query (encoder's hidden state)\n","    self.W1 = tf.keras.layers.Dense(attention_units)\n","    # Dense layer for value (encoder's outputs)\n","    self.W2 = tf.keras.layers.Dense(attention_units)\n","    # Dense layer to compute attention score\n","    self.V = tf.keras.layers.Dense(1)\n","\n","  def call(self, query, values):\n","    # query: hidden state\n","    # query shape == (batch_size, rnn_units)\n","    # values shape == (batch_size, encoder's input length, rnn_units)\n","    \n","    # query shape == (batch_size, 1, rnn_units)\n","    query = tf.expand_dims(query, 1)\n","\n","    # x shape == (batch_size, encoder's input length, attention_units)\n","    x = tf.nn.tanh(self.W1(query) + self.W2(values))\n","    # attention_score shape == (batch_size, encoder's input length, 1)\n","    attention_score = self.V(x)\n","\n","    # attention_weights shape == (batch_size, encoder's input length, 1)\n","    attention_weights = tf.nn.softmax(attention_score, axis=1)\n","\n","    # context_vector shape after sum == (batch_size, encoder's input length)\n","    context_vector = attention_weights * values\n","    context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","    return context_vector"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"vwZJrP6bATI-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":272},"executionInfo":{"status":"ok","timestamp":1597545632539,"user_tz":240,"elapsed":6665,"user":{"displayName":"Hung Do","photoUrl":"","userId":"04577237231470330040"}},"outputId":"fee71615-1738-4ec0-a860-aa88fbec3601"},"source":["class Encoder(tf.keras.Model):\n","  def __init__(self, rnn_units, embedding_dim, vocab_size, batch_size = 1):\n","    super().__init__()\n","    self.batch_size = batch_size\n","    self.rnn_units = rnn_units\n","    self.embedding_dim = embedding_dim\n","    self.vocab_size = vocab_size\n","    self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim, mask_zero=True, name='encoder_embedding')\n","    self.LSTM = tf.keras.layers.LSTM(self.rnn_units, return_state=True, return_sequences=True, name='encoder_lstm')\n","\n","  def call(self, input , hidden_state, cell_state):\n","    x = self.embedding(input)\n","    states = [hidden_state, cell_state]\n","    encoder_outputs, hidden_state, cell_state = self.LSTM(x, initial_state=states)\n","    \n","    return encoder_outputs, hidden_state, cell_state\n","\n","  def get_initial_state(self):\n","    hidden_state = tf.zeros((self.batch_size, self.rnn_units))\n","    cell_state = tf.zeros((self.batch_size, self.rnn_units))\n","    \n","    return hidden_state, cell_state \n","\n","encoder = Encoder(RNN_UNITS, EMBEDDING_DIM, VOCAB_SIZE, BATCH_SIZE)\n","states = encoder.get_initial_state()\n","\n","print('--- Decoder example ---')\n","print('Input: {}'.format(list(dataset.take(1).as_numpy_iterator())[0][0].shape))\n","encoder_output, encoder_h_state, encoder_c_state = encoder(list(dataset.take(1).as_numpy_iterator())[0][0], states[0], states[1])\n","print('Output: {}'.format(encoder_output.shape))\n","encoder.summary()"],"execution_count":6,"outputs":[{"output_type":"stream","text":["--- Decoder example ---\n","Input: (50, 140)\n","Output: (50, 140, 200)\n","Model: \"encoder\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","encoder_embedding (Embedding multiple                  1316200   \n","_________________________________________________________________\n","encoder_lstm (LSTM)          multiple                  320800    \n","=================================================================\n","Total params: 1,637,000\n","Trainable params: 1,637,000\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ny3oSP7AdEXf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":272},"executionInfo":{"status":"ok","timestamp":1597545633054,"user_tz":240,"elapsed":7165,"user":{"displayName":"Hung Do","photoUrl":"","userId":"04577237231470330040"}},"outputId":"6dffe2d4-fe9b-4415-a96c-375b0e218362"},"source":["class Decoder(tf.keras.Model):\n","  def __init__(self, rnn_units, embedding_dim, vocab_size, batch_size = 1):\n","    super().__init__()\n","    self.batch_size = batch_size\n","    self.rnn_units = rnn_units\n","    self.embedding_dim = embedding_dim\n","    self.vocab_size = vocab_size\n","    self.attention = Attention(attention_units = rnn_units)\n","    self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim, mask_zero=True, name='decoder_embedding')\n","    self.LSTM = tf.keras.layers.LSTM(self.rnn_units , return_state=True , return_sequences=True, name='decoder_lstm')\n","    self.dense = tf.keras.layers.Dense(self.vocab_size, activation=tf.keras.activations.softmax, name='decoder_output')\n","    \n","  def call(self, query, value, input, hidden_state, cell_state):\n","    \"\"\"\n","    query: hidden states got after passing through the encoder's LSTM layer\n","    value: predicted value at each time step in the encoder's LSTM layer\n","    input: batch of sequences \n","    hidden_state: current hidden state of the decoder\n","    cell_state: current cell state of the decoder \n","    \n","    \n","    \"\"\"   \n","    # attention_vector shape (batch_size, encoder's input length)\n","    attention_vector = self.attention(query, value)\n","\n","    # embedded shape (batch_size, decoder's input length, embedding dim)\n","    embedded = self.embedding(input)\n","    \n","    # x shape (batch size, 1, encoder's input length + embedding dim)\n","    x = tf.concat([tf.expand_dims(attention_vector, 1), embedded], axis=-1)\n","\n","    # lstm_outputs shape (batch_size, 1, rnn_units)\n","    states = [hidden_state, cell_state]\n","    lstm_outputs, hidden_state, cell_state = self.LSTM(x, initial_state = states)\n","    \n","    # softmax_outputs shape (batch_size, 1, vocab_size)\n","    softmax_outputs = self.dense(lstm_outputs)\n","\n","    return softmax_outputs, hidden_state, cell_state\n","\n","  def get_initial_state(self):\n","    hidden_state = tf.zeros((self.batch_size, self.rnn_units))\n","    cell_state = tf.zeros((self.batch_size, self.rnn_units))\n","\n","    return hidden_state, cell_state\n","  \n","decoder = Decoder(RNN_UNITS, EMBEDDING_DIM, VOCAB_SIZE, BATCH_SIZE)\n","states = decoder.get_initial_state()\n","\n","# The following line is just to enable summary\n","print('--- Encoder example ---')\n","print('Input: {}'.format(list(dataset.take(1).as_numpy_iterator())[0][1].shape))\n","\n","# query shape = (batch size, rnn_units*2)\n","query = tf.concat([encoder_h_state, encoder_c_state], axis=-1)\n","\n","output, decoder_h_state, decoder_c_state = decoder(query, encoder_output ,tf.expand_dims(list(dataset.take(1).as_numpy_iterator())[0][1][:,0],1), states[0], states[1])\n","print('Output: {}'.format(output.shape))\n","encoder.summary()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["--- Encoder example ---\n","Input: (50, 171)\n","Output: (50, 1, 6581)\n","Model: \"encoder\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","encoder_embedding (Embedding multiple                  1316200   \n","_________________________________________________________________\n","encoder_lstm (LSTM)          multiple                  320800    \n","=================================================================\n","Total params: 1,637,000\n","Trainable params: 1,637,000\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iqu9CLJqfh5C","colab_type":"text"},"source":["# **Training**"]},{"cell_type":"code","metadata":{"id":"ACf40Nc-DZ2n","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597546319871,"user_tz":240,"elapsed":1271,"user":{"displayName":"Hung Do","photoUrl":"","userId":"04577237231470330040"}},"outputId":"b84835f8-635c-4fbd-c940-1e94411209c6"},"source":["optimizer = tf.keras.optimizers.Adam()\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","# Directory where the checkpoints will be saved\n","checkpoint_dir = './drive/My Drive/data/attention_checkpoint'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n","checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd0d06a90f0>"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"Heci-tR3foyc","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597545633057,"user_tz":240,"elapsed":7141,"user":{"displayName":"Hung Do","photoUrl":"","userId":"04577237231470330040"}}},"source":["@tf.function\n","def train_step(encoder_input, decoder_input, target):\n","  batch_loss = 0\n","\n","  with tf.GradientTape() as tape:\n","    states = encoder.get_initial_state()\n","    encoder_output, encoder_h_state, encoder_c_state = encoder(encoder_input, states[0], states[1])\n","\n","    h_state = encoder_h_state\n","    c_state = encoder_c_state\n","        \n","    for timestep in range(decoder_input.shape[1]):\n","      # query shape = (batch size, rnn_units*2)\n","      query = tf.concat([h_state, c_state], axis=-1)\n","      \n","      output, h_state, c_state = decoder(query, encoder_output , tf.expand_dims(decoder_input[:,timestep], 1), h_state, c_state)\n","\n","      batch_loss += loss(target[:, timestep], output)\n","  \n","  variables = encoder.trainable_variables + decoder.trainable_variables\n","  gradients = tape.gradient(batch_loss, variables)\n","  optimizer.apply_gradients(zip(gradients, variables))\n","\n","  return batch_loss"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"ayhOR3nKfwCz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"4a408e52-73b1-4e3f-ad48-f8d68ad15227"},"source":["EPOCHS = 100\n","\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  total_loss = 0\n","\n","  for encoder_input, decoder_input, target in dataset:\n","    batch_loss = train_step(encoder_input, decoder_input, target)\n","    total_loss += batch_loss\n","    break\n","\n","  checkpoint.save(file_prefix = checkpoint_prefix)\n","\n","  print('Epoch {} Loss {}'.format(epoch + 1, total_loss))\n","  print('Time: {}'.format(time.time() - start))\n","  print()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1 Loss 1344.38037109375\n","Time: 0.8943362236022949\n","\n","Epoch 2 Loss 1341.560791015625\n","Time: 0.7919578552246094\n","\n","Epoch 3 Loss 1343.2802734375\n","Time: 0.7840390205383301\n","\n","Epoch 4 Loss 1341.8006591796875\n","Time: 1.0197832584381104\n","\n","Epoch 5 Loss 1345.3399658203125\n","Time: 0.830535888671875\n","\n","Epoch 6 Loss 1342.0201416015625\n","Time: 0.8389163017272949\n","\n","Epoch 7 Loss 1345.3394775390625\n","Time: 0.8129622936248779\n","\n","Epoch 8 Loss 1342.840087890625\n","Time: 0.8030567169189453\n","\n","Epoch 9 Loss 1345.0794677734375\n","Time: 0.8653566837310791\n","\n","Epoch 10 Loss 1343.260009765625\n","Time: 0.8288493156433105\n","\n","Epoch 11 Loss 1343.899658203125\n","Time: 0.8747529983520508\n","\n","Epoch 12 Loss 1342.7799072265625\n","Time: 0.8178226947784424\n","\n","Epoch 13 Loss 1345.7991943359375\n","Time: 0.9236540794372559\n","\n","Epoch 14 Loss 1345.2593994140625\n","Time: 1.3760418891906738\n","\n","Epoch 15 Loss 1345.3790283203125\n","Time: 1.1136293411254883\n","\n","Epoch 16 Loss 1344.6990966796875\n","Time: 1.478161334991455\n","\n","Epoch 17 Loss 1344.4791259765625\n","Time: 1.5707695484161377\n","\n","Epoch 18 Loss 1343.8394775390625\n","Time: 1.1212007999420166\n","\n","Epoch 19 Loss 1343.559326171875\n","Time: 1.1048190593719482\n","\n","Epoch 20 Loss 1343.7391357421875\n","Time: 0.8532788753509521\n","\n","Epoch 21 Loss 1343.5191650390625\n","Time: 0.8177857398986816\n","\n","Epoch 22 Loss 1342.2393798828125\n","Time: 1.0070006847381592\n","\n","Epoch 23 Loss 1343.1790771484375\n","Time: 0.8439359664916992\n","\n","Epoch 24 Loss 1342.4991455078125\n","Time: 0.9927377700805664\n","\n","Epoch 25 Loss 1343.6588134765625\n","Time: 0.8549351692199707\n","\n","Epoch 26 Loss 1343.5189208984375\n","Time: 0.8227999210357666\n","\n","Epoch 27 Loss 1342.5389404296875\n","Time: 0.9212737083435059\n","\n","Epoch 28 Loss 1342.638916015625\n","Time: 1.2407026290893555\n","\n","Epoch 29 Loss 1341.6593017578125\n","Time: 0.9821386337280273\n","\n","Epoch 30 Loss 1345.05810546875\n","Time: 0.8632102012634277\n","\n","Epoch 31 Loss 1342.3187255859375\n","Time: 0.8426856994628906\n","\n","Epoch 32 Loss 1342.7186279296875\n","Time: 1.1131198406219482\n","\n","Epoch 33 Loss 1341.998779296875\n","Time: 0.904456615447998\n","\n","Epoch 34 Loss 1346.837646484375\n","Time: 0.850045919418335\n","\n","Epoch 35 Loss 1342.5184326171875\n","Time: 0.8866071701049805\n","\n","Epoch 36 Loss 1344.278076171875\n","Time: 0.9582445621490479\n","\n","Epoch 37 Loss 1343.01806640625\n","Time: 0.9548308849334717\n","\n","Epoch 38 Loss 1343.47802734375\n","Time: 0.9358541965484619\n","\n","Epoch 39 Loss 1344.6177978515625\n","Time: 0.8534915447235107\n","\n","Epoch 40 Loss 1342.2982177734375\n","Time: 0.9450130462646484\n","\n","Epoch 41 Loss 1342.9580078125\n","Time: 0.91591477394104\n","\n","Epoch 42 Loss 1344.0977783203125\n","Time: 0.8901357650756836\n","\n","Epoch 43 Loss 1347.8167724609375\n","Time: 1.0226831436157227\n","\n","Epoch 44 Loss 1342.6175537109375\n","Time: 1.0149877071380615\n","\n","Epoch 45 Loss 1344.4373779296875\n","Time: 0.9281389713287354\n","\n","Epoch 46 Loss 1345.0372314453125\n","Time: 0.8214833736419678\n","\n","Epoch 47 Loss 1344.8973388671875\n","Time: 0.9841971397399902\n","\n","Epoch 48 Loss 1344.3375244140625\n","Time: 1.57612943649292\n","\n","Epoch 49 Loss 1343.057373046875\n","Time: 1.510277271270752\n","\n","Epoch 50 Loss 1343.17724609375\n","Time: 1.6886582374572754\n","\n","Epoch 51 Loss 1342.1575927734375\n","Time: 1.4297971725463867\n","\n","Epoch 52 Loss 1345.9967041015625\n","Time: 1.6198148727416992\n","\n","Epoch 53 Loss 1345.1368408203125\n","Time: 1.4016978740692139\n","\n","Epoch 54 Loss 1342.0972900390625\n","Time: 0.9489860534667969\n","\n","Epoch 55 Loss 1346.5565185546875\n","Time: 0.9948713779449463\n","\n","Epoch 56 Loss 1342.59716796875\n","Time: 1.9618332386016846\n","\n","Epoch 57 Loss 1342.9969482421875\n","Time: 1.0473124980926514\n","\n","Epoch 58 Loss 1345.9364013671875\n","Time: 1.2802925109863281\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"l40E11MkJMo-","colab_type":"text"},"source":["# **Evaluate**"]},{"cell_type":"code","metadata":{"id":"MWhsYqdk-gc9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597546172431,"user_tz":240,"elapsed":817,"user":{"displayName":"Hung Do","photoUrl":"","userId":"04577237231470330040"}},"outputId":"b73271c2-67c8-44da-8150-a59864ea215e"},"source":["test_encoder = Encoder(RNN_UNITS, EMBEDDING_DIM, VOCAB_SIZE, 1)\n","test_decoder = Decoder(RNN_UNITS, EMBEDDING_DIM, VOCAB_SIZE, 1)\n","test_checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=test_encoder, decoder=test_decoder)\n","test_checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd0d06c36a0>"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"Kgc-NzvVJYOl","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597546191553,"user_tz":240,"elapsed":515,"user":{"displayName":"Hung Do","photoUrl":"","userId":"04577237231470330040"}}},"source":["def generate_text(question):\n","  states = test_encoder.get_initial_state()\n","  \n","  # Converting our question to numbers (vectorizing)\n","  encoder_input = tokenizer.texts_to_sequences([question])\n","  encoder_input = pad_sequences(encoder_input, maxlen=max_encoder_len, padding='post')\n","  \n","  encoder_output, encoder_h_state, encoder_c_state = test_encoder(encoder_input, states[0], states[1])\n","  \n","  h_state = encoder_h_state\n","  c_state = encoder_c_state\n","  decoder_input = np.zeros((1, 1)) # starting with an empty string\n","  decoder_input[0, 0] = tokenizer.texts_to_sequences('<start>')[0][0]\n","  \n","  response_generated = []\n","  temperature = 1\n","  count = 0\n","  \n","  while True:\n","    query = tf.concat([h_state, c_state], axis=-1)\n","    predictions, h_state, c_state = test_decoder(query, encoder_output , decoder_input, h_state, c_state)\n","    predictions = tf.squeeze(predictions, [1])\n","\n","    # using a categorical distribution to predict the character returned by the model\n","    predictions = predictions / temperature\n","    predicted_id = np.argmax(predictions[0])\n","\n","    if tokenizer.sequences_to_texts([[predicted_id]])[0] != '<eov>':\n","      count +=1\n","      de_input = tf.expand_dims([predicted_id], 0)\n","      response_generated.append(tokenizer.sequences_to_texts([[predicted_id]])[0])\n","      \n","      if count == 10:\n","        break  \n","    else:\n","      response_generated.append('\\n')\n","      break\n","  \n","  return (' '.join(response_generated))"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"_AYnFN55JdtC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1597546201504,"user_tz":240,"elapsed":7230,"user":{"displayName":"Hung Do","photoUrl":"","userId":"04577237231470330040"}},"outputId":"24fff830-c655-4990-8d42-591137a66737"},"source":["question = input('Customer says:')\n","response = generate_text(question)\n","print('Assistant says: {}'.format(response))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Customer says:what time\n","Assistant says: <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n"],"name":"stdout"}]}]}